{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OpenPipe/ART/blob/main/examples/mcp-rl/mcp-rl-alphavantage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caZYLROd8xnV"
      },
      "source": [
        "To train a model for your custom task, click _Runtime_ and press _Run all_. Make sure you've enabled a free Tesla T4 GPU!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "**MCPâ€¢RL: Tool-driven agent training**\n",
        "\n",
        "This notebook shows how to train a Qwen 2.5 7B model to automatically optimize against any MCP server. Simply define the server's tools and resources and the notebook below will:\n",
        "\n",
        "1. Query the server's tools and resources\n",
        "2. Generate diverse input examples for your task\n",
        "3. Train the model using RULER's automatic evaluation\n",
        "4. Test the trained model on new inputs against the server\n",
        "\n",
        "RULER learns what makes a good output purely from the MCP server's tools and resources - no expected outputs required!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OsrwCDQ5cviC"
      },
      "outputs": [],
      "source": [
        "# @title ðŸ’¿ Installation\n",
        "\n",
        "!uv pip install -q openpipe-art==0.3.11.post2 langchain-core tenacity mcp>=1.11.0 \"gql<4\" --prerelease allow --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8b8kgQ69ZDM"
      },
      "source": [
        "<a name=\"Configuration\"></a>\n",
        "\n",
        "### ðŸŽ¯ Configuration - Edit These Settings\n",
        "\n",
        "Add an OpenRouter key and customize your training by modifying the values below.\n",
        "\n",
        "By default your model will be trained to retrieve and analyze stock and crypto market data from the Alphavantage MCP server. To teach your model another skill, set `TASK_DESCRIPTION` to one of the descriptions under **Advanced Settings**, or write your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so6r1_OG9en3"
      },
      "outputs": [],
      "source": [
        "# Required - Used for generating training inputs and RULER evaluation\n",
        "OPENROUTER_API_KEY = \"\"\n",
        "\n",
        "# Optional - Enables metric logging\n",
        "WANDB_API_KEY = \"\"\n",
        "\n",
        "# Shared key for the demo - DO NOT USE IN PRODUCTION, AND EXPECT RATE LIMITS\n",
        "ALPHAVANTAGE_API_KEY = \"HR32X84C3B4HJ93C\"\n",
        "\n",
        "# Choose the base model to train\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # Options: \"Qwen/Qwen2.5-3B-Instruct\", \"Qwen/Qwen2.5-7B-Instruct\", etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I_AFDSOv_LrB"
      },
      "outputs": [],
      "source": [
        "# @title Advanced Settings\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"mcp-7b-alphavantage\"  # Name for your trained model\n",
        "PROJECT_NAME = \"mcp-rl\"  # Project name for tracking\n",
        "\n",
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"num_training_inputs\": 16,  # Number of training inputs to generate\n",
        "    \"groups_per_step\": 2,  # Inputs to process per training step\n",
        "    \"num_epochs\": 3,  # Number of times through all data\n",
        "    \"rollouts_per_group\": 3,  # Different responses per input (for RULER comparison)\n",
        "    \"learning_rate\": 1e-5,  # Learning rate\n",
        "    \"max_training_steps\": None,  # Maximum training steps (set to None for no limit)\n",
        "}\n",
        "\n",
        "MAX_TURNS = 10  # Maximum number of turns for the model to generate during one rollout\n",
        "\n",
        "NUM_TEST_INPUTS = 8  # Number of test inputs to generate\n",
        "RULER_MODEL = \"openrouter/openai/o4-mini\"  # Model for RULER evaluation\n",
        "INPUT_GENERATION_MODEL = \"openai/o4-mini\"\n",
        "\n",
        "# GPU configuration (for T4 â€”Â keep these as-is unless you have a reason to change them)\n",
        "MAX_SEQ_LENGTH = 4096  # Maximum sequence length\n",
        "GPU_MEMORY_UTILIZATION = 0.7  # GPU memory usage (0.0-1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Alphavantage MCP server\n",
        "\n",
        "\n",
        "import asyncio\n",
        "from typing import Any, Dict, Optional\n",
        "import aiohttp\n",
        "import click\n",
        "import mcp.types as types\n",
        "from mcp.server.lowlevel import Server\n",
        "from mcp import StdioServerParameters\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_exponential,\n",
        "    retry_if_exception_type,\n",
        ")\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "\n",
        "class AlphaVantageClient:\n",
        "    \"\"\"Client for interacting with Alpha Vantage API\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "    async def fetch_data(self, function: str, **params) -> Dict[str, Any]:\n",
        "        \"\"\"Fetch data from Alpha Vantage API\"\"\"\n",
        "        query_params = {\n",
        "            \"function\": function,\n",
        "            \"apikey\": self.api_key,\n",
        "            \"datatype\": \"json\",\n",
        "            **params,\n",
        "        }\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.get(self.base_url, params=query_params) as response:\n",
        "                if response.status != 200:\n",
        "                    raise Exception(f\"API request failed: {response.status}\")\n",
        "\n",
        "                data = await response.json()\n",
        "\n",
        "                if \"Error Message\" in data:\n",
        "                    raise Exception(f\"Alpha Vantage API Error: {data['Error Message']}\")\n",
        "\n",
        "                if (\n",
        "                    \"Thank you for using Alpha Vantage! Please contact premium@alphavantage.co if you are targeting a higher API call volume.\"\n",
        "                    in data\n",
        "                ):\n",
        "                    raise Exception(\n",
        "                        \"Alpha Vantage API Error: Thank you for using Alpha Vantage! Please contact premium@alphavantage.co if you are targeting a higher API call volume.\"\n",
        "                    )\n",
        "\n",
        "                return data\n",
        "\n",
        "\n",
        "@click.command()\n",
        "@click.option(\"--api-key\", help=\"Alpha Vantage API key\", envvar=\"ALPHAVANTAGE_API_KEY\")\n",
        "@click.option(\"--port\", default=8000, help=\"Port to listen on for SSE\")\n",
        "@click.option(\n",
        "    \"--transport\",\n",
        "    type=click.Choice([\"stdio\", \"sse\"]),\n",
        "    default=\"stdio\",\n",
        "    help=\"Transport type\",\n",
        ")\n",
        "def main(api_key: Optional[str], port: int, transport: str) -> int:\n",
        "    if not api_key:\n",
        "        click.echo(\n",
        "            \"Error: Alpha Vantage API key is required. Set ALPHAVANTAGE_API_KEY environment variable or use --api-key option.\"\n",
        "        )\n",
        "        return 1\n",
        "\n",
        "    app = Server(\"mcp-alphavantage\")\n",
        "    client = AlphaVantageClient(api_key)\n",
        "\n",
        "    @app.list_tools()\n",
        "    async def list_tools() -> list[types.Tool]:\n",
        "        return [\n",
        "            types.Tool(\n",
        "                name=\"get_stock_quote\",\n",
        "                description=\"Get real-time stock quote for a symbol\",\n",
        "                inputSchema={\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"symbol\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Stock symbol (e.g., AAPL, MSFT)\",\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"symbol\"],\n",
        "                },\n",
        "            ),\n",
        "            types.Tool(\n",
        "                name=\"get_time_series_daily\",\n",
        "                description=\"Get daily time series data for a stock\",\n",
        "                inputSchema={\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"symbol\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Stock symbol (e.g., AAPL, MSFT)\",\n",
        "                        },\n",
        "                        \"outputsize\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Output size: compact (latest 100 data points)\",\n",
        "                            \"enum\": [\"compact\"],\n",
        "                            \"default\": \"compact\",\n",
        "                        },\n",
        "                    },\n",
        "                    \"required\": [\"symbol\"],\n",
        "                },\n",
        "            ),\n",
        "            types.Tool(\n",
        "                name=\"search_symbol\",\n",
        "                description=\"Search for stock symbols by keywords\",\n",
        "                inputSchema={\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"keywords\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Keywords to search for (e.g., company name)\",\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"keywords\"],\n",
        "                },\n",
        "            ),\n",
        "            types.Tool(\n",
        "                name=\"get_company_overview\",\n",
        "                description=\"Get fundamental data and company overview\",\n",
        "                inputSchema={\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"symbol\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Stock symbol (e.g., AAPL, MSFT)\",\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"symbol\"],\n",
        "                },\n",
        "            ),\n",
        "            types.Tool(\n",
        "                name=\"get_sma\",\n",
        "                description=\"Get Simple Moving Average (SMA) technical indicator\",\n",
        "                inputSchema={\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"symbol\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Stock symbol (e.g., AAPL, MSFT)\",\n",
        "                        },\n",
        "                        \"interval\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Time interval\",\n",
        "                            \"enum\": [\n",
        "                                \"1min\",\n",
        "                                \"5min\",\n",
        "                                \"15min\",\n",
        "                                \"30min\",\n",
        "                                \"60min\",\n",
        "                                \"daily\",\n",
        "                                \"weekly\",\n",
        "                                \"monthly\",\n",
        "                            ],\n",
        "                            \"default\": \"daily\",\n",
        "                        },\n",
        "                        \"time_period\": {\n",
        "                            \"type\": \"integer\",\n",
        "                            \"description\": \"Number of data points for SMA calculation\",\n",
        "                            \"default\": 30,\n",
        "                        },\n",
        "                        \"series_type\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Price type to use for calculation\",\n",
        "                            \"enum\": [\"close\", \"open\", \"high\", \"low\"],\n",
        "                            \"default\": \"close\",\n",
        "                        },\n",
        "                    },\n",
        "                    \"required\": [\"symbol\"],\n",
        "                },\n",
        "            ),\n",
        "            types.Tool(\n",
        "                name=\"get_rsi\",\n",
        "                description=\"Get Relative Strength Index (RSI) technical indicator\",\n",
        "                inputSchema={\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"symbol\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Stock symbol (e.g., AAPL, MSFT)\",\n",
        "                        },\n",
        "                        \"interval\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Time interval\",\n",
        "                            \"enum\": [\n",
        "                                \"daily\",\n",
        "                                \"weekly\",\n",
        "                                \"monthly\",\n",
        "                            ],\n",
        "                            \"default\": \"daily\",\n",
        "                        },\n",
        "                        \"time_period\": {\n",
        "                            \"type\": \"integer\",\n",
        "                            \"description\": \"Number of data points for RSI calculation\",\n",
        "                            \"default\": 14,\n",
        "                        },\n",
        "                        \"series_type\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Price type to use for calculation\",\n",
        "                            \"enum\": [\"close\", \"open\", \"high\", \"low\"],\n",
        "                            \"default\": \"close\",\n",
        "                        },\n",
        "                    },\n",
        "                    \"required\": [\"symbol\"],\n",
        "                },\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "    @app.call_tool()\n",
        "    @retry(\n",
        "        stop=stop_after_attempt(5),\n",
        "        wait=wait_exponential(multiplier=1, min=1, max=30),\n",
        "        retry=retry_if_exception_type(\n",
        "            (aiohttp.ClientError, asyncio.TimeoutError, Exception)\n",
        "        ),\n",
        "    )\n",
        "    async def call_tool(name: str, arguments: dict) -> list[types.TextContent]:\n",
        "        try:\n",
        "            if name == \"get_stock_quote\":\n",
        "                data = await client.fetch_data(\n",
        "                    \"GLOBAL_QUOTE\", symbol=arguments[\"symbol\"]\n",
        "                )\n",
        "                return [\n",
        "                    types.TextContent(\n",
        "                        type=\"text\",\n",
        "                        text=f\"Stock Quote for {arguments['symbol']}:\\n{_format_json(data)}\",\n",
        "                    )\n",
        "                ]\n",
        "\n",
        "            elif name == \"get_time_series_daily\":\n",
        "                outputsize = arguments.get(\"outputsize\", \"compact\")\n",
        "                data = await client.fetch_data(\n",
        "                    \"TIME_SERIES_DAILY\",\n",
        "                    symbol=arguments[\"symbol\"],\n",
        "                    outputsize=outputsize,\n",
        "                )\n",
        "                return [\n",
        "                    types.TextContent(\n",
        "                        type=\"text\",\n",
        "                        text=f\"Daily Time Series for {arguments['symbol']}:\\n{_format_json(data)}\",\n",
        "                    )\n",
        "                ]\n",
        "\n",
        "            elif name == \"search_symbol\":\n",
        "                data = await client.fetch_data(\n",
        "                    \"SYMBOL_SEARCH\", keywords=arguments[\"keywords\"]\n",
        "                )\n",
        "                return [\n",
        "                    types.TextContent(\n",
        "                        type=\"text\",\n",
        "                        text=f\"Symbol Search Results for '{arguments['keywords']}':\\n{_format_json(data)}\",\n",
        "                    )\n",
        "                ]\n",
        "\n",
        "            elif name == \"get_company_overview\":\n",
        "                data = await client.fetch_data(\"OVERVIEW\", symbol=arguments[\"symbol\"])\n",
        "                return [\n",
        "                    types.TextContent(\n",
        "                        type=\"text\",\n",
        "                        text=f\"Company Overview for {arguments['symbol']}:\\n{_format_json(data)}\",\n",
        "                    )\n",
        "                ]\n",
        "\n",
        "            elif name == \"get_sma\":\n",
        "                data = await client.fetch_data(\n",
        "                    \"SMA\",\n",
        "                    symbol=arguments[\"symbol\"],\n",
        "                    interval=arguments.get(\"interval\", \"daily\"),\n",
        "                    time_period=arguments.get(\"time_period\", 30),\n",
        "                    series_type=arguments.get(\"series_type\", \"close\"),\n",
        "                )\n",
        "                tech_analysis_key = \"Technical Analysis: SMA\"\n",
        "                time_period = arguments.get(\"time_period\", 30)\n",
        "                # Alpha Vantage returns a dict keyed by timestamp; convert to list to slice\n",
        "                data[tech_analysis_key] = dict(\n",
        "                    list(data[tech_analysis_key].items())[:time_period]\n",
        "                )\n",
        "                return [\n",
        "                    types.TextContent(\n",
        "                        type=\"text\",\n",
        "                        text=f\"SMA for {arguments['symbol']}:\\n{_format_json(data)}\",\n",
        "                    )\n",
        "                ]\n",
        "\n",
        "            elif name == \"get_rsi\":\n",
        "                data = await client.fetch_data(\n",
        "                    \"RSI\",\n",
        "                    symbol=arguments[\"symbol\"],\n",
        "                    interval=arguments.get(\"interval\", \"daily\"),\n",
        "                    time_period=arguments.get(\"time_period\", 14),\n",
        "                    series_type=arguments.get(\"series_type\", \"close\"),\n",
        "                )\n",
        "                tech_analysis_key = \"Technical Analysis: RSI\"\n",
        "                time_period = arguments.get(\"time_period\", 14)\n",
        "                # Alpha Vantage returns a dict keyed by timestamp; convert to list to slice\n",
        "                data[tech_analysis_key] = dict(\n",
        "                    list(data[tech_analysis_key].items())[:time_period]\n",
        "                )\n",
        "                return [\n",
        "                    types.TextContent(\n",
        "                        type=\"text\",\n",
        "                        text=f\"RSI for {arguments['symbol']}:\\n{_format_json(data)}\",\n",
        "                    )\n",
        "                ]\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown tool: {name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            return [types.TextContent(type=\"text\", text=f\"Error: {str(e)}\")]\n",
        "\n",
        "    if transport == \"sse\":\n",
        "        from mcp.server.sse import SseServerTransport\n",
        "        from starlette.applications import Starlette\n",
        "        from starlette.responses import Response\n",
        "        from starlette.routing import Mount, Route\n",
        "\n",
        "        sse = SseServerTransport(\"/messages/\")\n",
        "\n",
        "        async def handle_sse(request):\n",
        "            async with sse.connect_sse(\n",
        "                request.scope, request.receive, request._send\n",
        "            ) as streams:\n",
        "                await app.run(\n",
        "                    streams[0], streams[1], app.create_initialization_options()\n",
        "                )\n",
        "            return Response()\n",
        "\n",
        "        starlette_app = Starlette(\n",
        "            debug=True,\n",
        "            routes=[\n",
        "                Route(\"/sse\", endpoint=handle_sse, methods=[\"GET\"]),\n",
        "                Mount(\"/messages/\", app=sse.handle_post_message),\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        import uvicorn\n",
        "\n",
        "        uvicorn.run(starlette_app, host=\"127.0.0.1\", port=port)\n",
        "    else:\n",
        "        from mcp.server.stdio import stdio_server\n",
        "\n",
        "        async def arun():\n",
        "            async with stdio_server() as streams:\n",
        "                await app.run(\n",
        "                    streams[0], streams[1], app.create_initialization_options()\n",
        "                )\n",
        "\n",
        "        asyncio.run(arun())\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "def _format_json(data: Dict[str, Any]) -> str:\n",
        "    \"\"\"Format JSON data for display\"\"\"\n",
        "    import json\n",
        "\n",
        "    return json.dumps(data, indent=2)\n",
        "\n",
        "\n",
        "server_params = StdioServerParameters(\n",
        "    command=\"python\",\n",
        "    args=[\n",
        "        \"servers/python/mcp_alphavantage/server.py\",\n",
        "        \"--api-key\",\n",
        "        os.getenv(\"ALPHAVANTAGE_API_KEY\", \"demo\"),\n",
        "    ],\n",
        "    env={\"ALPHAVANTAGE_API_KEY\": os.getenv(\"ALPHAVANTAGE_API_KEY\")},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Let's generate our train and validation scenarios!\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from typing import List, Dict, Any\n",
        "from mcp import ClientSession\n",
        "from mcp.client.stdio import stdio_client\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "async def generate_scenarios(\n",
        "    num_scenarios: int = 24,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    # Connect to MCP server to get available tools and resources\n",
        "    async with stdio_client(server_params) as (read, write):\n",
        "        async with ClientSession(read, write) as session:\n",
        "            # Initialize the connection\n",
        "            await session.initialize()\n",
        "\n",
        "            # Get available tools\n",
        "            tools_result = await session.list_tools()\n",
        "            tools_info = []\n",
        "            for tool in tools_result.tools:\n",
        "                tool_info = {\n",
        "                    \"name\": tool.name,\n",
        "                    \"description\": tool.description,\n",
        "                    \"parameters\": tool.inputSchema,\n",
        "                }\n",
        "                tools_info.append(tool_info)\n",
        "\n",
        "            # Get available resources\n",
        "            try:\n",
        "                resources_result = await session.list_resources()\n",
        "                resources_info = []\n",
        "                for resource in resources_result.resources:\n",
        "                    resource_info = {\n",
        "                        \"uri\": str(resource.uri),\n",
        "                        \"name\": resource.name,\n",
        "                        \"description\": resource.description,\n",
        "                        \"mimeType\": resource.mimeType,\n",
        "                    }\n",
        "                    resources_info.append(resource_info)\n",
        "            except Exception:\n",
        "                # Some servers might not have resources\n",
        "                resources_info = []\n",
        "\n",
        "    # Prepare the prompt for o3\n",
        "    tools_description = json.dumps(tools_info, indent=2)\n",
        "    resources_description = (\n",
        "        json.dumps(resources_info, indent=2)\n",
        "        if resources_info\n",
        "        else \"No resources available\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"You are an expert at creating realistic scenarios for testing AI agents that interact with MCP (Model Context Protocol) servers.\n",
        "\n",
        "Given the following available tools and resources from an MCP server, generate {num_scenarios} diverse, realistic scenarios that a user might want to accomplish using these tools.\n",
        "\n",
        "AVAILABLE TOOLS:\n",
        "{tools_description}\n",
        "\n",
        "AVAILABLE RESOURCES:\n",
        "{resources_description}\n",
        "\n",
        "Requirements for scenarios:\n",
        "1. Each scenario should be a task that can be accomplished using the available tools\n",
        "2. Scenarios should vary in complexity - some simple (1-2 tool calls), some complex (multiple tool calls)\n",
        "3. Scenarios should cover different use cases and tool combinations (though the task should not specify which tools to use)\n",
        "4. Each scenario should be realistic - something a real user might actually want to do\n",
        "5. Assign a difficulty rating from 1 (easy, single tool call) to 5 (hard, complex multi-step analysis)\n",
        "6. The task should always include generating a summary of the work done and a thorough analysis and report of the results\n",
        "\n",
        "You must respond with a JSON object containing a \"scenarios\" array of exactly {num_scenarios} objects. Each object must have:\n",
        "- \"task\": string describing the scenario\n",
        "- \"difficulty\": integer from 1-5 representing complexity\n",
        "\n",
        "Example:\n",
        "{{\n",
        "  \"scenarios\": [\n",
        "    {{\"task\": \"Get the current stock price for Apple (AAPL)\", \"difficulty\": 1}},\n",
        "    {{\"task\": \"Compare the 30-day SMA with current price for Tesla and determine if it's above or below the moving average and generate a thorough analysis and report\", \"difficulty\": 2}}\n",
        "  ]\n",
        "}}\"\"\"\n",
        "\n",
        "    # Call OpenAI's model with structured JSON output\n",
        "    client = openai.OpenAI(\n",
        "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "    )\n",
        "\n",
        "    # Define the JSON schema for the response\n",
        "    response_schema = {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"scenarios\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"task\": {\"type\": \"string\"},\n",
        "                        \"difficulty\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 5},\n",
        "                    },\n",
        "                    \"required\": [\"task\", \"difficulty\"],\n",
        "                    \"additionalProperties\": False,\n",
        "                },\n",
        "                \"minItems\": num_scenarios,\n",
        "                \"maxItems\": num_scenarios,\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"scenarios\"],\n",
        "        \"additionalProperties\": False,\n",
        "    }\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=INPUT_GENERATION_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_completion_tokens=4000,\n",
        "        response_format={\n",
        "            \"type\": \"json_schema\",\n",
        "            \"json_schema\": {\"name\": \"scenario_list\", \"schema\": response_schema},\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Parse the JSON response\n",
        "    content = response.choices[0].message.content\n",
        "    result = json.loads(content)\n",
        "\n",
        "    # Extract scenarios from the response\n",
        "    if \"scenarios\" in result:\n",
        "        scenarios = result[\"scenarios\"]\n",
        "    else:\n",
        "        # If the response is just an array\n",
        "        scenarios = result if isinstance(result, list) else list(result.values())[0]\n",
        "\n",
        "    # Validate we got exactly the right number\n",
        "    if len(scenarios) != num_scenarios:\n",
        "        raise ValueError(f\"Expected {num_scenarios} scenarios, got {len(scenarios)}\")\n",
        "\n",
        "    return scenarios\n",
        "\n",
        "\n",
        "num_scenarios = TRAINING_CONFIG[\"num_training_inputs\"] + NUM_TEST_INPUTS\n",
        "for _ in range(10):\n",
        "    scenarios = await generate_scenarios(\n",
        "        server_params,\n",
        "        num_scenarios=num_scenarios,\n",
        "    )\n",
        "\n",
        "    if len(scenarios) == num_scenarios:\n",
        "        break\n",
        "\n",
        "\n",
        "print(f\"\\nGenerated {len(scenarios)} scenarios:\")\n",
        "for i, scenario in enumerate(scenarios, 1):\n",
        "    print(f\"{i}. Task: {scenario['task']}\")\n",
        "    print(f\"   Difficulty: {scenario['difficulty']}/5\")\n",
        "\n",
        "# Shuffle scenarios randomly\n",
        "random.shuffle(scenarios)\n",
        "\n",
        "raw_train_scenarios = scenarios[:TRAINING_CONFIG[\"num_training_inputs\"]]\n",
        "raw_val_scenarios = scenarios[TRAINING_CONFIG[\"num_training_inputs\"]:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Run this cell to train your model!\n",
        "\n",
        "import art\n",
        "from art.utils import iterate_dataset\n",
        "from art.local import LocalBackend\n",
        "from art.ruler import ruler_score_group\n",
        "from dataclasses import dataclass\n",
        "import weave\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "import mcp.types as types\n",
        "from openai import AsyncOpenAI\n",
        "import json\n",
        "\n",
        "# Required\n",
        "if OPENROUTER_API_KEY:\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = OPENROUTER_API_KEY\n",
        "else:\n",
        "    raise ValueError(\n",
        "        \"OPENROUTER_API_KEY is required for data generation and RULER evaluation.\"\n",
        "    )\n",
        "\n",
        "# Optional\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "    weave.init(PROJECT_NAME)\n",
        "else:\n",
        "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")\n",
        "\n",
        "# Required for Alphavantage demo\n",
        "if ALPHAVANTAGE_API_KEY:\n",
        "    os.environ[\"ALPHAVANTAGE_API_KEY\"] = ALPHAVANTAGE_API_KEY\n",
        "else:\n",
        "    raise ValueError(\n",
        "        \"ALPHAVANTAGE_API_KEY is required for the Alphavantage demo.\"\n",
        "    )\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=MODEL_NAME,\n",
        "    project=PROJECT_NAME,\n",
        "    base_model=BASE_MODEL,\n",
        ")\n",
        "\n",
        "# To run on a T4, we need to override some config defaults.\n",
        "model._internal_config = art.dev.InternalModelConfig(\n",
        "    init_args=art.dev.InitArgs(\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "    ),\n",
        "    engine_args=art.dev.EngineArgs(\n",
        "        enforce_eager=True,\n",
        "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize the server\n",
        "backend = LocalBackend(\n",
        "    in_process=True,\n",
        "    path=\"./.art\",\n",
        ")\n",
        "\n",
        "# Register the model with the local Backend\n",
        "await model.register(backend)\n",
        "\n",
        "print(\"Model created!\")\n",
        "print(\"Base model:\", BASE_MODEL)\n",
        "print(\"Model name:\", MODEL_NAME)\n",
        "print(\"Project name:\", PROJECT_NAME)\n",
        "\n",
        "# =============== Rollout function code ===============\n",
        "\n",
        "def get_content_text(result: types.CallToolResult) -> str:\n",
        "    # Extract text content from MCP result\n",
        "    if hasattr(result, \"content\") and result.content:\n",
        "        if isinstance(result.content, list):\n",
        "            # Handle list of content items\n",
        "            content_text = \"\"\n",
        "            for item in result.content:\n",
        "                if isinstance(item, types.TextContent):\n",
        "                    content_text += item.text\n",
        "                else:\n",
        "                    content_text += str(item)\n",
        "        elif isinstance(result.content[0], types.TextContent):\n",
        "            content_text = result.content[0].text\n",
        "        else:\n",
        "            content_text = str(result.content)\n",
        "    else:\n",
        "        content_text = str(result)\n",
        "\n",
        "    return content_text\n",
        "\n",
        "@dataclass\n",
        "class McpScenario:\n",
        "    \"\"\"A scenario for MCP agent evaluation.\"\"\"\n",
        "\n",
        "    task_description: str\n",
        "    server_params: StdioServerParameters\n",
        "    max_turns: int = 10\n",
        "\n",
        "\n",
        "@weave.op()\n",
        "async def rollout(\n",
        "    model: art.Model,\n",
        "    scenario: McpScenario,\n",
        "    debug: bool = False,\n",
        ") -> art.Trajectory:\n",
        "    \"\"\"Run an MCP agent rollout with server parameters.\n",
        "\n",
        "    Args:\n",
        "        model: The ART model to use for the agent\n",
        "        scenario: The MCP scenario to run (must include server_params)\n",
        "\n",
        "    Returns:\n",
        "        Trajectory containing the results of the rollout\n",
        "    \"\"\"\n",
        "    traj = art.Trajectory(\n",
        "        messages_and_choices=[],\n",
        "        reward=0,\n",
        "        metadata={\"task\": scenario.task_description},\n",
        "        metrics={\n",
        "            \"task_completed\": False,\n",
        "            \"success\": False,\n",
        "            \"ran_out_of_turns\": False,\n",
        "        },\n",
        "        scenario=scenario,\n",
        "    )\n",
        "\n",
        "    # Initialize system prompt\n",
        "    system_prompt = f\"\"\"You are an MCP (Model Context Protocol) agent.\\n\\nYou have access to MCP tools through the server. Use them to complete your task.\\n\\nWhen you believe you have completed the task, call the 'complete_task' function with a summary of what you accomplished. You have a total of {scenario.max_turns} turns to complete the task. Only use tool calls, do not write any content. After you have completed the task, call the 'complete_task' function with a summary of what you accomplished. Call complete_task by itself, not in conjunction with any other tools.\"\"\"\n",
        "\n",
        "    # Connect to MCP server using stdio\n",
        "    try:\n",
        "        async with stdio_client(scenario.server_params) as (read, write):\n",
        "            async with ClientSession(read, write) as session:\n",
        "                # Initialize the connection\n",
        "                await session.initialize()\n",
        "\n",
        "                # Get available tools from the server\n",
        "                tools_result = await session.list_tools()\n",
        "\n",
        "                # Convert to OpenAI format\n",
        "                tool_schemas = []\n",
        "                for tool in tools_result.tools:\n",
        "                    tool_schema = {\n",
        "                        \"type\": \"function\",\n",
        "                        \"function\": {\n",
        "                            \"name\": tool.name,\n",
        "                            \"description\": tool.description or f\"MCP tool: {tool.name}\",\n",
        "                            \"parameters\": tool.inputSchema\n",
        "                            or {\"type\": \"object\", \"properties\": {}},\n",
        "                        },\n",
        "                    }\n",
        "                    tool_schemas.append(tool_schema)\n",
        "\n",
        "                if debug:\n",
        "                    available_tools = [\n",
        "                        tool[\"function\"][\"name\"] for tool in tool_schemas\n",
        "                    ]\n",
        "                    print(f\"Available MCP tools: {available_tools}\")\n",
        "\n",
        "                # Add completion tool schema\n",
        "                tool_schemas.append(\n",
        "                    {\n",
        "                        \"type\": \"function\",\n",
        "                        \"function\": {\n",
        "                            \"name\": \"complete_task\",\n",
        "                            \"description\": \"Complete the task with a summary\",\n",
        "                            \"parameters\": {\n",
        "                                \"type\": \"object\",\n",
        "                                \"properties\": {\n",
        "                                    \"summary\": {\n",
        "                                        \"type\": \"string\",\n",
        "                                        \"description\": \"Summary of accomplishments\",\n",
        "                                    }\n",
        "                                },\n",
        "                                \"required\": [\"summary\"],\n",
        "                            },\n",
        "                        },\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                traj.tools = tool_schemas\n",
        "\n",
        "                # Initialize conversation\n",
        "                traj.messages_and_choices = [\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"Please complete this task: {scenario.task_description}\",\n",
        "                    },\n",
        "                ]\n",
        "\n",
        "                if debug:\n",
        "                    print(traj.messages())\n",
        "\n",
        "                num_turns = 0\n",
        "                task_completed = False\n",
        "\n",
        "                # Main interaction loop\n",
        "                while num_turns < scenario.max_turns and not task_completed:\n",
        "                    num_turns += 1\n",
        "\n",
        "                    try:\n",
        "                        # Get LLM response\n",
        "                        async with traj.track_duration(\"llm_completion\"):\n",
        "                            client = AsyncOpenAI(\n",
        "                                api_key=model.inference_api_key,\n",
        "                                base_url=model.inference_base_url,\n",
        "                            )\n",
        "\n",
        "                            response = await client.chat.completions.create(\n",
        "                                model=model.inference_model_name\n",
        "                                if model.inference_model_name\n",
        "                                else model.name,\n",
        "                                messages=traj.messages(),\n",
        "                                tools=tool_schemas,\n",
        "                                max_completion_tokens=4000,\n",
        "                            )\n",
        "\n",
        "                        choice = response.choices[0]\n",
        "\n",
        "                        if debug:\n",
        "                            print(f\"Choice: {choice.message}\")\n",
        "\n",
        "                        traj.messages_and_choices.append(choice)\n",
        "\n",
        "                        # Handle tool calls\n",
        "                        if choice.message.tool_calls:\n",
        "                            for tool_call in choice.message.tool_calls:\n",
        "                                try:\n",
        "                                    tool_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "                                    if tool_call.function.name == \"complete_task\":\n",
        "                                        traj.metrics[\"task_completed\"] = True\n",
        "                                        traj.log(\n",
        "                                            f\"Task completion attempted with summary: {tool_args['summary']}\"\n",
        "                                        )\n",
        "                                    else:\n",
        "                                        # Call MCP tool through session\n",
        "                                        result = await session.call_tool(\n",
        "                                            tool_call.function.name, tool_args\n",
        "                                        )\n",
        "\n",
        "                                        content_text = get_content_text(result)\n",
        "\n",
        "                                        if len(content_text) > 20000:\n",
        "                                            print(\n",
        "                                                f\"Tool call result for {tool_call.function.name} is too long: {len(content_text)}\"\n",
        "                                            )\n",
        "                                            print(f\"Args: {tool_args}\")\n",
        "                                            # print first and last 1000 characters\n",
        "                                            print(content_text[:1000])\n",
        "                                            print(content_text[-1000:])\n",
        "                                            raise Exception(\n",
        "                                                f\"Tool call result for {tool_call.function.name} is too long: {len(content_text)}\"\n",
        "                                            )\n",
        "\n",
        "                                        # Add tool response\n",
        "                                        traj.messages_and_choices.append(\n",
        "                                            {\n",
        "                                                \"role\": \"tool\",\n",
        "                                                \"tool_call_id\": tool_call.id,\n",
        "                                                \"content\": content_text,\n",
        "                                            }\n",
        "                                        )\n",
        "\n",
        "                                    if debug:\n",
        "                                        print(f\"Tool call result: {content_text}\")\n",
        "\n",
        "                                except Exception as e:\n",
        "                                    traj.log(f\"Tool call error: {e}\")\n",
        "\n",
        "                                    # Add error response\n",
        "                                    traj.messages_and_choices.append(\n",
        "                                        {\n",
        "                                            \"role\": \"tool\",\n",
        "                                            \"tool_call_id\": tool_call.id,\n",
        "                                            \"content\": f\"Error: {str(e)}\",\n",
        "                                        }\n",
        "                                    )\n",
        "                        else:\n",
        "                            # No tool calls, just continue conversation\n",
        "                            break\n",
        "\n",
        "                    except Exception as e:\n",
        "                        traj.log(f\"Error in turn {num_turns}: {e}\")\n",
        "                        break\n",
        "\n",
        "    except Exception as e:\n",
        "        traj.log(f\"MCP server error: {e}\")\n",
        "    if not task_completed and num_turns == scenario.max_turns:\n",
        "        traj.metrics[\"ran_out_of_turns\"] = True\n",
        "\n",
        "    traj.metrics[\"num_turns\"] = num_turns\n",
        "\n",
        "    if debug:\n",
        "        for message in traj.messages_and_choices:\n",
        "            print(\"\\n\")\n",
        "            print(message)\n",
        "            print(\"\\n\")\n",
        "\n",
        "    return traj.finish()\n",
        "\n",
        "# =============== Training code ===============\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Get configuration from model config or use defaults\n",
        "config = getattr(model, \"config\", None)\n",
        "\n",
        "if config is None:\n",
        "    raise ValueError(\"Model config is required\")\n",
        "\n",
        "print(\n",
        "    f\"Using config: max_turns={MAX_TURNS}, rollouts_per_group={TRAINING_CONFIG['rollouts_per_group']}, groups_per_step={TRAINING_CONFIG['groups_per_step']}, num_epochs={TRAINING_CONFIG['num_epochs']}, learning_rate={TRAINING_CONFIG['learning_rate']}\"\n",
        ")\n",
        "\n",
        "await model.register(backend)\n",
        "\n",
        "train_scenarios = [\n",
        "    McpScenario(\n",
        "        task_description=scenario[\"task\"],\n",
        "        server_params=server_params,\n",
        "        max_turns=MAX_TURNS,\n",
        "    )\n",
        "    for scenario in raw_train_scenarios\n",
        "]\n",
        "\n",
        "# Create dataset iterator using raw scenarios (not McpScenario objects)\n",
        "train_iterator = iterate_dataset(\n",
        "    train_scenarios,\n",
        "    groups_per_step=TRAINING_CONFIG[\"groups_per_step\"],\n",
        "    num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),  # Resume from checkpoint\n",
        ")\n",
        "\n",
        "# Main training loop using iterate_dataset\n",
        "for batch in train_iterator:\n",
        "    print(\"Gathering trajectory groups with RULER scoring...\")\n",
        "\n",
        "    # Use gather_trajectory_groups with ruler_score_group\n",
        "    groups = await art.gather_trajectory_groups(\n",
        "        (\n",
        "            art.TrajectoryGroup(\n",
        "                rollout(model, scenario, False)\n",
        "                for _ in range(TRAINING_CONFIG[\"rollouts_per_group\"])\n",
        "            )\n",
        "            for scenario in batch.items\n",
        "        ),\n",
        "        pbar_desc=f\"train gather step {batch.step}\",\n",
        "        after_each=lambda group: ruler_score_group(\n",
        "            group,\n",
        "            judge_model=RULER_MODEL,\n",
        "            debug=True,  # Show judge reasoning\n",
        "            swallow_exceptions=True,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    print(\"starting train\")\n",
        "    await model.train(groups, config=art.TrainConfig(learning_rate=TRAINING_CONFIG[\"learning_rate\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YRO9ndqo5ky4"
      },
      "outputs": [],
      "source": [
        "# @title Test Your Model!\n",
        "\n",
        "# Generate test inputs\n",
        "print(\"Generating test inputs...\")\n",
        "val_scenarios = [\n",
        "    McpScenario(\n",
        "        task_description=scenario[\"task\"],\n",
        "        server_params=server_params,\n",
        "        max_turns=MAX_TURNS,\n",
        "    )\n",
        "    for scenario in raw_train_scenarios\n",
        "]\n",
        "\n",
        "print(f\"\\nðŸ§ª Testing the trained model on {len(val_scenarios)} new inputs:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, scenario in enumerate(val_scenarios):\n",
        "    print(f\"\\nTest {i + 1}:\")\n",
        "    print(f\"Input: {scenario.task_description}\")\n",
        "\n",
        "    # Run the model\n",
        "    result_trajectory = await rollout(model, scenario)\n",
        "\n",
        "    # Extract the model's response\n",
        "    messages = result_trajectory.messages()\n",
        "    model_response = messages[-1][\"content\"] if messages else \"No response\"\n",
        "\n",
        "    print(f\"Model output: {model_response}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\nðŸŽ‰ Testing completed!\")\n",
        "print(f\"\\nYour model '{MODEL_NAME}' has been trained to effectively use the Alphavantage MCP server.\")\n",
        "print(\"\\nTo use this model in production:\")\n",
        "print(\"1. The model checkpoint is saved in ./.art/\")\n",
        "print(\"2. You can load it using the vLLM library\")\n",
        "print(\n",
        "    \"3. Or continue training with more examples by adjusting the configuration at the top\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "utI-VYM8s5lo"
      },
      "outputs": [],
      "source": [
        "# @title Upload to Hugging Face ðŸ¤—\n",
        "\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "lora_model_path = (\n",
        "    f\".art/{model.project}/models/{model.name}/{await model.get_step():04d}\"\n",
        ")\n",
        "\n",
        "peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=lora_model_path,\n",
        "    max_seq_length=16384,\n",
        "    dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "if False:  # Change to True to upload finetune\n",
        "    peft_model.push_to_hub_merged(f\"HF_ACCOUNT/{model.name}\", tokenizer, token=\"hf_...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuevYgXT-I1h"
      },
      "source": [
        "### Next Steps\n",
        "\n",
        "Congratulations! You've successfully trained a custom model for your task using only:\n",
        "- A pre-built MCP server\n",
        "- Example inputs (no outputs needed!)\n",
        "- RULER's automatic evaluation\n",
        "\n",
        "Here are some ways to improve results:\n",
        "\n",
        "1. **More diverse inputs**: Generate more varied input examples\n",
        "2. **Longer training**: Increase the number of training steps\n",
        "3. **More comparisons**: Increase `rollouts_per_group` for better RULER comparisons\n",
        "4. **MCP server refinement**: Add better tools and resources to the server\n",
        "5. **Hyperparameter tuning**: Adjust learning rate, batch size, etc.\n",
        "\n",
        "Remember: RULER learns what \"good\" means from your MCP server alone - no labeled data required!\n",
        "\n",
        "For more advanced use cases, check out the [ART documentation](https://art.openpipe.ai)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
